{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor # To share lru_cache\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from os.path import join\n",
    "from ocamcamera import OcamCamera\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "from models import OmniMVS\n",
    "from models import SphericalSweeping\n",
    "from dataloader import OmniStereoDataset\n",
    "from dataloader import load_image, load_invdepth\n",
    "from torchvision import transforms\n",
    "from dataloader.custom_transforms import Resize, ToTensor, Normalize\n",
    "from utils import InvDepthConverter, evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Training for OmniMVS',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument('root_dir', metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('-t','--train-list', default='../datasets/omnithings/omnithings_train.txt',\n",
    "                    type=str, help='Text file includes filenames for training')\n",
    "parser.add_argument('--epochs', default=30, type=int, metavar='N', help='total epochs')\n",
    "parser.add_argument('--pretrained', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained model')\n",
    "                   \n",
    "parser.add_argument('-b', '--batch-size', default=1, type=int, metavar='N', help='mini-batch size')\n",
    "\n",
    "parser.add_argument('--min_depth', type=float, default=0.55, help='minimum depth in m')\n",
    "if False:\n",
    "    # Paper setting\n",
    "    parser.add_argument('--ndisp', type=int, default=192, help='number of disparity')\n",
    "    parser.add_argument('--input_width', type=int, default=800, help='input image width')\n",
    "    parser.add_argument('--input_height', type=int, default=768, help='input image height')\n",
    "    parser.add_argument('--output_width', type=int, default=640, help='output depth width')\n",
    "    parser.add_argument('--output_height', type=int, default=320, help='output depth height')\n",
    "else:\n",
    "    # Light weight\n",
    "    parser.add_argument('--ndisp', type=int, default=64, help='number of disparity')\n",
    "    parser.add_argument('--input_width', type=int, default=500, help='input image width')\n",
    "    parser.add_argument('--input_height', type=int, default=480, help='input image height')\n",
    "    parser.add_argument('--output_width', type=int, default=512, help='output depth width')\n",
    "    parser.add_argument('--output_height', type=int, default=256, help='output depth height')\n",
    "parser.add_argument('-j', '--workers', default=6, type=int, metavar='J', help='number of data loading workers')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-3, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',help='momentum for sgd')\n",
    "parser.add_argument('--arch', default='omni_small', type=str, help='architecture name for log folder')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='L', help='tensorboard log interval')\n",
    "                   \n",
    "\n",
    "# a, b = 800, 768\n",
    "# for it in range(b+1)[::-1]:\n",
    "#     new_a = it*a/b\n",
    "#     if new_a == int(new_a):\n",
    "#         print(f'{new_a:.0f}', it)\n",
    "# print()\n",
    "# a, b = 640, 320\n",
    "# for it in range(b+1)[::-1]:\n",
    "#     new_a = it*a/b\n",
    "#     if new_a == int(new_a) and min(new_a,it)%32==0:\n",
    "#         print(f'{new_a:.0f}', it)\n",
    "\n",
    "root_dir = '../datasets/omnithings'\n",
    "file_list = '-t ./omnithings_train.txt'\n",
    "# model_params = \"--input_width 500 --input_height 480 --output_width 512 --output_height 256 --ndisp 64\"\n",
    "pretrained = \"--pretrained ../lowlr_smalldisp_0111-2204/checkpoints_20.pth\"\n",
    "args = parser.parse_args(f'{root_dir} {file_list} {pretrained} --lr 1e-3 --ndisp 48'.split()) #\n",
    "\n",
    "# Generate filename list\n",
    "with open('omnithings_train.txt', 'w') as f:\n",
    "    for i in range(1, 4097):\n",
    "        f.write(f'{i:05}.png\\n')\n",
    "    for i in range(5121, 8241):\n",
    "        f.write(f'{i:05}.png\\n')\n",
    "with open('omnithings_val.txt', 'w') as f:\n",
    "    for i in range(8241, 10240+1):\n",
    "        f.write(f'{i:05}.png\\n')\n",
    "with open('omnihouse_val.txt', 'w') as f:\n",
    "    for i in range(1, 2560+1):\n",
    "        f.write(f'{i:04}.png\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "if device.type != 'cpu':\n",
    "    cudnn.benchmark = True\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = SphericalSweeping(args.root_dir, h=args.output_height, w=args.output_width)\n",
    "model = OmniMVS(sweep, args.ndisp, args.min_depth, h=args.output_height, w=args.output_width)\n",
    "invd_0 = model.inv_depths[0]\n",
    "invd_max = model.inv_depths[-1]\n",
    "\n",
    "converter = InvDepthConverter(args.ndisp, invd_0, invd_max)\n",
    "model = model.to(device)\n",
    "start_epoch = 0\n",
    "\n",
    "# cache\n",
    "num_cam = 4\n",
    "pool = ThreadPoolExecutor(5)\n",
    "futures = []\n",
    "for i in range(num_cam):\n",
    "    for d in model.depths[::2]:\n",
    "        futures.append(pool.submit(sweep.get_grid, i, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup solver scheduler\n",
    "print('=> setting optimizer')\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr, momentum=args.momentum)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "\n",
    "print('=> setting scheduler')\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "if args.pretrained:\n",
    "    checkpoint = torch.load(args.pretrained)\n",
    "    param_check = {\n",
    "        'ndisp' : model.ndisp,\n",
    "        'min_depth' : model.min_depth,\n",
    "        'output_width' : model.w,\n",
    "        'output_height' : model.h,\n",
    "    }\n",
    "    for key, val in param_check.items():\n",
    "        if not checkpoint[key] == val:\n",
    "            print(f'Error! Key:{key} is not the same as the checkpoints')\n",
    "            \n",
    "    print(\"=> using pre-trained weights\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(\"=> Resume training from epoch {}\".format(start_epoch))\n",
    "    \n",
    "timestamp = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "log_folder = join('checkpoints', f'{args.arch}_{timestamp}')\n",
    "print(f'=> create log folder: {log_folder}')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "with open(join(log_folder, 'args.json'), 'w') as f:\n",
    "    json.dump(vars(args), f, indent=1)\n",
    "writer = SummaryWriter(log_dir=log_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup transform\n",
    "image_size = (args.input_width, args.input_height)\n",
    "depth_size = (args.output_width, args.output_height)\n",
    "\n",
    "ToPIL = lambda x: transforms.ToPILImage()(x.cpu())\n",
    "train_transform = transforms.Compose([Resize(image_size, depth_size), ToTensor(), Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_txt = args.train_list\n",
    "root_dir = args.root_dir\n",
    "trainset = OmniStereoDataset(root_dir, filename_txt, transform=train_transform)\n",
    "print(f'{len(trainset)} samples were found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "loader_iter = iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = loader_iter.next()\n",
    "tensor = batch['cam1'][0]\n",
    "plt.imshow(ToPIL(0.5+0.5*tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd = batch['idepth'][0]\n",
    "plt.imshow(invd.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=> wait for a while until all tasks in pool are finished')\n",
    "pool.shutdown()\n",
    "print('=> Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cache\n",
    "# with torch.no_grad():\n",
    "#     for key in batch.keys():\n",
    "#         batch[key] = batch[key].to(device)\n",
    "# #     out = model(batch)\n",
    "# # #     del out, batch # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single batch overfitting\n",
    "# from tqdm.notebook import tqdm\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # collect few batch\n",
    "# batchs = []\n",
    "# for it in train_loader:\n",
    "#     batchs.append(it)\n",
    "#     if len(batchs) > 10:\n",
    "#         break\n",
    "        \n",
    "# # Start overfitting\n",
    "# model.train()\n",
    "# losses = []\n",
    "# pbar = tqdm(range(1000))\n",
    "# for it in pbar:\n",
    "#     batch = random.choice(batchs)\n",
    "#     # to cuda\n",
    "#     for key in batch.keys():\n",
    "#         batch[key] = batch[key].to(device)\n",
    "#     pred = model(batch)\n",
    "\n",
    "#     gt_idepth = batch['idepth']\n",
    "#     # Loss function\n",
    "#     gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "#     loss = nn.L1Loss()(pred, gt_invd_idx)\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "#     # update parameters\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # update progress bar\n",
    "#     display = OrderedDict(it=f\"{it:>2}\", loss=f\"{losses[-1]:.4f}\")\n",
    "#     pbar.set_postfix(display)\n",
    "\n",
    "# plt.title('Loss (log)')\n",
    "# plt.plot(losses)\n",
    "# plt.yscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # to cuda\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        pred = model(batch)\n",
    "\n",
    "        gt_idepth = batch['idepth']\n",
    "        # Loss function  \n",
    "        gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "        loss = nn.L1Loss()(pred, gt_invd_idx)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        display = OrderedDict(epoch=f\"{epoch:>2}\",loss=f\"{losses[-1]:.4f}\")\n",
    "        pbar.set_postfix(display)\n",
    "        \n",
    "        # tensorboard log\n",
    "        if idx % args.log_interval == 0:\n",
    "            niter = epoch*len(train_loader)+idx\n",
    "            writer.add_scalar('train/loss', loss.item(), niter)\n",
    "        if idx % 100*args.log_interval == 0:\n",
    "            niter = epoch*len(train_loader)+idx\n",
    "            imgs = []\n",
    "            for cam in model.cam_list:\n",
    "                imgs.append(0.5*batch[cam][0]+0.5)\n",
    "            img_grid = make_grid(imgs, nrow=2, padding=5, pad_value=1)\n",
    "            writer.add_image('train/fisheye', img_grid, niter)\n",
    "            writer.add_image('train/pred', pred/model.ndisp, niter)\n",
    "            writer.add_image('train/gt', gt_invd_idx/model.ndisp, niter)\n",
    "    \n",
    "    # End of one epoch\n",
    "    scheduler.step()\n",
    "    ave_loss = sum(losses)/len(losses)\n",
    "    writer.add_scalar('train/loss_ave', ave_loss, epoch)\n",
    "    print(f\"Epoch:{epoch}, Loss average:{ave_loss:.4f}\")\n",
    "    \n",
    "    save_data = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'ave_loss' : ave_loss,\n",
    "        'ndisp' : model.ndisp,\n",
    "        'min_depth' : model.min_depth,\n",
    "        'output_width' : model.w,\n",
    "        'output_height' : model.h,\n",
    "    }\n",
    "    torch.save(save_data, join(log_folder, f'checkpoints_{epoch}.pth'))\n",
    "    \n",
    "#     plt.title(f'epoch {epoch}:Loss (log)')\n",
    "#     plt.plot(losses)\n",
    "#     plt.yscale('log')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    root_dir = '../datasets/omnithings'\n",
    "    filename_list = 'omnithings_val.txt'\n",
    "else:\n",
    "    root_dir = '../datasets/omnihouse'\n",
    "    filename_list = 'omnihouse_val.txt'\n",
    "    \n",
    "valset = OmniStereoDataset(root_dir, filename_list, transform=train_transform)\n",
    "val_loader = DataLoader(valset, args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "loader_iter = iter(val_loader)\n",
    "# loader_iter = iter(train_loader)\n",
    "print(filename_list)\n",
    "print('val_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = batchs[3]#\n",
    "batch = loader_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "    pred = model(batch)\n",
    "    gt_idepth = batch['idepth']\n",
    "    gt_invd_idx = converter.invdepth_to_index(gt_idepth, round_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for cam in model.cam_list:\n",
    "    imgs.append(0.5*batch[cam][0]+0.5)\n",
    "img_grid = ToPIL(make_grid(imgs, padding=5, pad_value=1))\n",
    "\n",
    "pred_vis = ToPIL(pred/args.ndisp)\n",
    "gt_vis = ToPIL(gt_invd_idx/args.ndisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap='viridis'\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12,12), subplot_kw=({\"xticks\":(), \"yticks\":()}))\n",
    "ax[0].set_title('fisheye images')\n",
    "ax[0].imshow(img_grid)\n",
    "ax[1].set_title('prediction')\n",
    "ax[1].imshow(pred_vis, cmap=cmap)\n",
    "ax[2].set_title('groudtruth')\n",
    "ax[2].imshow(gt_vis, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "preds = []\n",
    "gts = []\n",
    "total = 512 # len(val_loader)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for idx, batch in tqdm(enumerate(val_loader), total=total):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        pred = model(batch)\n",
    "        gt_idepth = batch['idepth']\n",
    "        gt_invd_idx = converter.invdepth_to_index(gt_idepth, round_value=False)\n",
    "        preds.append(pred.cpu())\n",
    "        gts.append(gt_invd_idx.cpu())\n",
    "\n",
    "    if len(gts) >= total:\n",
    "        preds = torch.cat(preds)\n",
    "        gts = torch.cat(gts)\n",
    "        break\n",
    "        \n",
    "errors, error_names = evaluation_metrics(preds, gts, args.ndisp)\n",
    "print(\"Error: \")\n",
    "print(\"{:>8}, {:>8}, {:>8}, {:>8}, {:>8}\".format(*error_names))\n",
    "print(\"{:8.4f}, {:8.4f}, {:8.4f}, {:8.4f}, {:8.4f}\".format(*errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------Experimental from here -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as Rot\n",
    "def convertPoseToOmniMVS(Twcs, filename):\n",
    "    rot_t_vecs = []\n",
    "    for T in Twcs:\n",
    "        # rot\n",
    "        R = T[:3,:3]\n",
    "        rotvec = Rot.from_matrix(R).as_rotvec()\n",
    "        # tvec m -> cm\n",
    "        tvec = T[:3, 3]*100\n",
    "        rot_t_vecs.append(np.concatenate((rotvec, tvec)))\n",
    "    rot_t_vecs = np.stack(rot_t_vecs)\n",
    "    np.savetxt(filename, rot_t_vecs, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to OmniMVS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load camera poses\n",
    "data_folder = \"../real_data/\"\n",
    "fov = 185\n",
    "fs_read = cv2.FileStorage(join(data_folder, \"final_camera_poses.yml\"), cv2.FILE_STORAGE_READ)\n",
    "Twcs = []\n",
    "for i in range(4):\n",
    "    # get world <- cam transformation\n",
    "    key = f'originimg{i}'\n",
    "    Twcs.append(fs_read.getNode(key).mat())\n",
    "    \n",
    "# ocamcalib filenames in data_folder\n",
    "ocam_files = [\n",
    "    'calib_results_0.txt',\n",
    "    'calib_results_1.txt',\n",
    "    'calib_results_2.txt',\n",
    "    'calib_results_3.txt'\n",
    "]\n",
    "img_files = [\n",
    "    'img0.jpg',\n",
    "    'img1.jpg',\n",
    "    'img2.jpg',\n",
    "    'img3.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to OmniMVS format\n",
    "convertPoseToOmniMVS(Twcs, join(data_folder, 'poses.txt'))\n",
    "\n",
    "# convert to OmniMVS filename\n",
    "import shutil\n",
    "for i, it in enumerate(ocam_files):\n",
    "    src = join(data_folder, it)\n",
    "    dst = join(data_folder, f'ocam{i+1}.txt')\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change sweeping module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sweep = SphericalSweeping(data_folder, h=model.h, w=model.w, fov=fov)\n",
    "model.sweep = new_sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([Resize((500, 500), depth_size), ToTensor(), Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "batch = {}\n",
    "for i in range(4):\n",
    "    cam = model.cam_list[i]\n",
    "    fname = join(data_folder, img_files[i])\n",
    "    valid = model.sweep.valid_area(i)\n",
    "    batch[cam] = load_image(fname, valid=valid)\n",
    "    \n",
    "batch = transform(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "        if batch[key].dim() == 3:\n",
    "            batch[key].unsqueeze_(0)\n",
    "    pred = model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for cam in model.cam_list:\n",
    "    imgs.append(0.5*batch[cam][0]+0.5)\n",
    "img_grid = ToPIL(make_grid(imgs, padding=5, pad_value=1))\n",
    "\n",
    "pred_vis = ToPIL(pred/args.ndisp)\n",
    "# gt_vis = ToPIL(gt_invd_idx/args.ndisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inputs = {}\n",
    "vis_outputs = {}\n",
    "\n",
    "def vis_hook(m, i, o, name):\n",
    "    vis_inputs[name] = i[0]\n",
    "    vis_outputs[name] = o\n",
    "    \n",
    "# add hook\n",
    "model.transference.register_forward_hook(partial(vis_hook, name='transference'))\n",
    "model.fusion.register_forward_hook(partial(vis_hook, name='fusion'))\n",
    "model.cost_regularization.register_forward_hook(partial(vis_hook, name='cost_reg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "    pred = model(batch)\n",
    "    gt_idepth = batch['idepth']\n",
    "    gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "    error = torch.abs(pred-gt_invd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inputs['transference'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd_idx = 9\n",
    "vis_tensor = vis_inputs['transference'][0, :, invd_idx].unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inputs['fusion'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd_idx = 0\n",
    "vis_tensor = vis_inputs['fusion'][0, :, invd_idx].unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1, normalize=True)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd_idx = 4\n",
    "vis_tensor = vis_outputs['fusion'][0, :, invd_idx].unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1, normalize=True)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_outputs['cost_reg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_tensor = vis_outputs['cost_reg'][0,0,:,:64,:].transpose(0, 1)\n",
    "vis_tensor = vis_tensor.unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1, normalize=True)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
