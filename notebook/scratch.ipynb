{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor # To share lru_cache\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../ocamcalib_undistort')\n",
    "sys.path.insert(0, '../')\n",
    "from ocamcamera import OcamCamera\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "from models import OmniMVS\n",
    "from models import SphericalSweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from os.path import join\n",
    "import cv2\n",
    "\n",
    "## Generate filename list\n",
    "# with open('omnithings_train.txt', 'w') as f:\n",
    "#     for i in range(1, 4097):\n",
    "#         f.write(f'{i:05}.png\\n')\n",
    "\n",
    "\n",
    "class OmniStereoDataset(Dataset):\n",
    "    \"\"\"Omnidirectional Stereo Dataset.\n",
    "    http://cvlab.hanyang.ac.kr/project/omnistereo/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, filename_txt, transform=None, fov=220):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # load filenames\n",
    "        with open(filename_txt) as f:\n",
    "            data = f.read()\n",
    "        self.filenames = data.strip().split('\\n')\n",
    "\n",
    "        \n",
    "        # folder name\n",
    "        self.cam_list = ['cam1', 'cam2', 'cam3', 'cam4']\n",
    "        self.depth_folder = 'depth_train_640'\n",
    "        \n",
    "        \n",
    "        # load ocam calibration data and generate valid image\n",
    "        self.ocams = []\n",
    "        self.valids = []\n",
    "        for cam in self.cam_list:\n",
    "            ocam_file = join(root_dir, f'o{cam}.txt')\n",
    "            self.ocams.append(OcamCamera(ocam_file, fov, show_flag=False))\n",
    "            self.valids.append(self.ocams[-1].valid_area())\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "    \n",
    "        filename = self.filenames[idx]\n",
    "        # load images\n",
    "        for i, cam in enumerate(self.cam_list):\n",
    "            img_path = join(self.root_dir, cam, filename)\n",
    "            sample[cam] = load_image(img_path, valid=self.valids[i])\n",
    "        # load inverse depth\n",
    "        depth_path = join(self.root_dir, self.depth_folder, filename)\n",
    "        sample['idepth'] = load_invdepth(depth_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "def load_invdepth(filename, min_depth=55):\n",
    "    '''\n",
    "    min_depth in [cm]\n",
    "    '''\n",
    "    invd_value = cv2.imread(filename, cv2.IMREAD_ANYDEPTH)\n",
    "    invdepth = (invd_value/100.0)/(min_depth*655)+np.finfo(np.float32).eps\n",
    "    invdepth *= 100 # unit conversion from cm to m\n",
    "    return invdepth\n",
    "\n",
    "\n",
    "def load_image(filename, gray=True, valid=None):\n",
    "    img = cv2.imread(filename)\n",
    "    if gray:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    if not valid is None:\n",
    "        img[valid==0] = 0\n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Training for OmniMVS',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument('root_dir', metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('-t','--train-list', default='../datasets/omnithings/omnithings_train.txt',\n",
    "                    type=str, help='Text file includes filenames for training')\n",
    "parser.add_argument('--epochs', default=30, type=int, metavar='N', help='total epochs')\n",
    "parser.add_argument('--pretrained', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained model')\n",
    "                   \n",
    "parser.add_argument('-b', '--batch-size', default=1, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--ndisp', type=int, default=192, help='number of disparity')\n",
    "parser.add_argument('--min_depth', type=float, default=0.55, help='minimum depth in m')\n",
    "parser.add_argument('--output_width', type=int, default=640, help='output depth width')\n",
    "parser.add_argument('--output_height', type=int, default=320, help='output depth height')\n",
    "parser.add_argument('-j', '--workers', default=6, type=int, metavar='N', help='number of data loading workers')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-3, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',help='momentum for sgd')\n",
    "                    \n",
    "args = parser.parse_args('../datasets/omnithings -t ./omnithings_train.txt --ndisp 28'.split()) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "if device.type != 'cpu':\n",
    "    cudnn.benchmark = True\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        self.cam_list = ['cam1', 'cam2', 'cam3', 'cam4']\n",
    "        self.depth = 'idepth'\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "    def __call__(self, sample):\n",
    "        # dataloader deal with conversion for others\n",
    "        sample[self.depth] = torch.from_numpy(sample[self.depth]).float()\n",
    "        for cam in self.cam_list:\n",
    "            sample[cam] = self.ToTensor(sample[cam])\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.cam_list = ['cam1', 'cam2', 'cam3', 'cam4']\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for cam in self.cam_list:\n",
    "            for t, m, s in zip(sample[cam], self.mean, self.std):\n",
    "                t.sub_(m).div_(s)\n",
    "        return sample\n",
    "    \n",
    "ToPIL = transforms.ToPILImage()\n",
    "train_transform = transforms.Compose([ToTensor(), Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_txt = args.train_list\n",
    "root_dir = args.root_dir\n",
    "trainset = OmniStereoDataset(root_dir, filename_txt, transform=train_transform)\n",
    "print(f'{len(trainset)} samples were found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, args.batch_size, shuffle=True, num_workers=args.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = iter(train_loader).next()\n",
    "tensor = batch['cam1'][0]\n",
    "plt.imshow(ToPIL(0.5+0.5*tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd = batch['idepth'][0]\n",
    "plt.imshow(invd.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvDepthConverter(object):\n",
    "    def __init__(self, ndisp, invd_0, invd_max):\n",
    "        self._ndisp = ndisp\n",
    "        self._invd_0 = invd_0\n",
    "        self._invd_max = invd_max\n",
    "        \n",
    "    def invdepth_to_index(self, idepth):\n",
    "        invd_idx = (self._ndisp-1)*(idepth - self._invd_0)/(self._invd_max - self._invd_0)\n",
    "        # Q: why round?\n",
    "        invd_idx = torch.round(invd_idx)\n",
    "        return invd_idx\n",
    "\n",
    "    def index_to_invdepth(self, invd_idx):\n",
    "        idepth = self.invd + invd_idx*(self._invd_max - self._invd_0)/(self._ndisp-1)\n",
    "        return idepth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = SphericalSweeping(root_dir, h=args.output_height, w=args.output_width)\n",
    "model = OmniMVS(sweep, args.ndisp, args.min_depth, h=args.output_height, w=args.output_width)\n",
    "invd_0 = model.inv_depths[0]\n",
    "invd_max = model.inv_depths[-1]\n",
    "\n",
    "converter = InvDepthConverter(args.ndisp, invd_0, invd_max)\n",
    "model = model.to(device)\n",
    "start_epoch = 0\n",
    "\n",
    "# cache\n",
    "num_cam = 4\n",
    "pool = ThreadPoolExecutor(5)\n",
    "futures = []\n",
    "for i in range(num_cam):\n",
    "    for d in model.depths[::2]:\n",
    "        futures.append(pool.submit(sweep.get_grid, i, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup solver scheduler\n",
    "print('=> setting optimizer')\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr, momentum=args.momentum)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "\n",
    "print('=> setting scheduler')\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "if args.pretrained:\n",
    "    checkpoint = torch.load(args.pretrained)\n",
    "    print(\"=> using pre-trained weights\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(\"=> Resume training from epoch {}\".format(start_epoch))\n",
    "    \n",
    "print('=> wait for a while until all tasks in pool are finished')\n",
    "pool.shutdown()\n",
    "print('=> Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cache\n",
    "# with torch.no_grad():\n",
    "#     for key in batch.keys():\n",
    "#         batch[key] = batch[key].to(device)\n",
    "# #     out = model(batch)\n",
    "# # #     del out, batch # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single batch overfitting\n",
    "# from tqdm.notebook import tqdm\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# batch = iter(train_loader).next()\n",
    "\n",
    "# losses = []\n",
    "# pbar = tqdm(range(1000))\n",
    "# for it in pbar:\n",
    "#     # to cuda\n",
    "#     for key in batch.keys():\n",
    "#         batch[key] = batch[key].to(device)\n",
    "#     pred = model(batch)\n",
    "\n",
    "#     gt_idepth = batch['idepth']\n",
    "#     # Loss function  \n",
    "#     gt_invindex = converter.invdepth_to_index(gt_idepth)\n",
    "#     loss = nn.L1Loss()(pred, gt_invindex)\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "#     # update parameters\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # update progress bar\n",
    "#     display = OrderedDict(it=f\"{it:>2}\",loss=f\"{losses[-1]:.4f}\")\n",
    "#     pbar.set_postfix(display)\n",
    "    \n",
    "# plt.title('Loss (log)')\n",
    "# plt.plot(losses)\n",
    "# plt.yscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # to cuda\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        pred = model(batch)\n",
    "\n",
    "        gt_idepth = batch['idepth']\n",
    "        # Loss function  \n",
    "        gt_invindex = converter.invdepth_to_index(gt_idepth)\n",
    "        loss = nn.L1Loss()(pred, gt_invindex)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        display = OrderedDict(epoch=f\"{epoch:>2}\",loss=f\"{losses[-1]:.4f}\")\n",
    "        pbar.set_postfix(display)\n",
    "    \n",
    "    # End of one epoch\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch:{epoch}, Loss average:{sum(losses)/len(losses):.4f}\")\n",
    "    \n",
    "    save_data = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'ave_loss' : sum(losses)/len(losses),\n",
    "    }\n",
    "    torch.save(save_data, f'checkpoints_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = loader_iter.next()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "    pred = model(batch)\n",
    "    gt_idepth = batch['idepth']\n",
    "    gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "    error = torch.abs(pred-gt_invd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToPIL(pred.cpu()/args.ndisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToPIL(gt_invd_idx.cpu()/args.ndisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
