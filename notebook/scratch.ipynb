{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor # To share lru_cache\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../ocamcalib_undistort')\n",
    "sys.path.insert(0, '../')\n",
    "from os.path import join\n",
    "from ocamcamera import OcamCamera\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "from models import OmniMVS\n",
    "from models import SphericalSweeping\n",
    "from dataloader import OmniStereoDataset\n",
    "from dataloader import load_image, load_invdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Training for OmniMVS',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument('root_dir', metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('-t','--train-list', default='../datasets/omnithings/omnithings_train.txt',\n",
    "                    type=str, help='Text file includes filenames for training')\n",
    "parser.add_argument('--epochs', default=30, type=int, metavar='N', help='total epochs')\n",
    "parser.add_argument('--pretrained', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained model')\n",
    "                   \n",
    "parser.add_argument('-b', '--batch-size', default=1, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--ndisp', type=int, default=192, help='number of disparity')\n",
    "parser.add_argument('--min_depth', type=float, default=0.55, help='minimum depth in m')\n",
    "parser.add_argument('--input_width', type=int, default=800, help='input image width')\n",
    "parser.add_argument('--input_height', type=int, default=768, help='input image height')\n",
    "parser.add_argument('--output_width', type=int, default=640, help='output depth width')\n",
    "parser.add_argument('--output_height', type=int, default=320, help='output depth height')\n",
    "parser.add_argument('-j', '--workers', default=6, type=int, metavar='J', help='number of data loading workers')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-3, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',help='momentum for sgd')\n",
    "parser.add_argument('--arch', default='omni_small', type=str, help='architecture name for log folder')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='L', help='tensorboard log interval')\n",
    "                   \n",
    "\n",
    "# a, b = 800, 768\n",
    "# for it in range(b+1)[::-1]:\n",
    "#     new_a = it*a/b\n",
    "#     if new_a == int(new_a):\n",
    "#         print(f'{new_a:.0f}', it)\n",
    "# a, b = 640, 320\n",
    "# for it in range(b+1)[::-1]:\n",
    "#     new_a = it*a/b\n",
    "#     if new_a == int(new_a) and min(new_a,it)%32==0:\n",
    "#         print(f'{new_a:.0f}', it)\n",
    "root_dir = '../datasets/omnithings'\n",
    "file_list = '-t ./omnithings_train.txt'\n",
    "resize_param = \"--input_width 500 --input_height 480 --output_width 448 --output_height 224\"\n",
    "pretrained = \"--pretrained ../omni_small_0111-1209/checkpoints_3.pth\"\n",
    "args = parser.parse_args(f'{root_dir} {resize_param} {pretrained} {file_list} --ndisp 48 --lr 1e-3'.split()) #\n",
    "\n",
    "# Generate filename list\n",
    "with open('omnithings_train.txt', 'w') as f:\n",
    "    for i in range(1, 4097):\n",
    "        f.write(f'{i:05}.png\\n')\n",
    "with open('omnithings_val.txt', 'w') as f:\n",
    "    for i in range(5121, 10240+1):\n",
    "        f.write(f'{i:05}.png\\n')\n",
    "with open('omnihouse_val.txt', 'w') as f:\n",
    "    for i in range(1, 2560+1):\n",
    "        f.write(f'{i:04}.png\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "if device.type != 'cpu':\n",
    "    cudnn.benchmark = True\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, image_size, depth_size):\n",
    "        self.cam_list = ['cam1', 'cam2', 'cam3', 'cam4']\n",
    "        self.depth = 'idepth'\n",
    "    def __call__(self, sample):\n",
    "        if self.depth in sample:\n",
    "            sample[self.depth] = cv2.resize(sample[self.depth], (depth_size))\n",
    "        for cam in self.cam_list:\n",
    "            sample[cam] = cv2.resize(sample[cam], (image_size))\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        self.cam_list = ['cam1', 'cam2', 'cam3', 'cam4']\n",
    "        self.depth = 'idepth'\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "    def __call__(self, sample):\n",
    "        if self.depth in sample:\n",
    "            sample[self.depth] = torch.from_numpy(sample[self.depth]).float()\n",
    "        for cam in self.cam_list:\n",
    "            sample[cam] = self.ToTensor(sample[cam])\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.cam_list = ['cam1', 'cam2', 'cam3', 'cam4']\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for cam in self.cam_list:\n",
    "            for t, m, s in zip(sample[cam], self.mean, self.std):\n",
    "                t.sub_(m).div_(s)\n",
    "        return sample\n",
    "    \n",
    "image_size = (args.input_width, args.input_height)\n",
    "depth_size = (args.output_width, args.output_height)\n",
    "\n",
    "ToPIL = lambda x:transforms.ToPILImage()(x.cpu())\n",
    "train_transform = transforms.Compose([Resize(image_size, depth_size), ToTensor(), Normalize()])\n",
    "# train_transform = transforms.Compose([ToTensor(), Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_txt = args.train_list\n",
    "root_dir = args.root_dir\n",
    "trainset = OmniStereoDataset(root_dir, filename_txt, transform=train_transform)\n",
    "print(f'{len(trainset)} samples were found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "loader_iter = iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = loader_iter.next()\n",
    "tensor = batch['cam1'][0]\n",
    "plt.imshow(ToPIL(0.5+0.5*tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd = batch['idepth'][0]\n",
    "plt.imshow(invd.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvDepthConverter(object):\n",
    "    def __init__(self, ndisp, invd_0, invd_max):\n",
    "        self._ndisp = ndisp\n",
    "        self._invd_0 = invd_0\n",
    "        self._invd_max = invd_max\n",
    "        \n",
    "    def invdepth_to_index(self, idepth):\n",
    "        invd_idx = (self._ndisp-1)*(idepth - self._invd_0)/(self._invd_max - self._invd_0)\n",
    "        # Q: why round?\n",
    "        invd_idx = torch.round(invd_idx)\n",
    "        return invd_idx\n",
    "\n",
    "    def index_to_invdepth(self, invd_idx):\n",
    "        idepth = self.invd + invd_idx*(self._invd_max - self._invd_0)/(self._ndisp-1)\n",
    "        return idepth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = SphericalSweeping(root_dir, h=args.output_height, w=args.output_width)\n",
    "model = OmniMVS(sweep, args.ndisp, args.min_depth, h=args.output_height, w=args.output_width)\n",
    "invd_0 = model.inv_depths[0]\n",
    "invd_max = model.inv_depths[-1]\n",
    "\n",
    "converter = InvDepthConverter(args.ndisp, invd_0, invd_max)\n",
    "model = model.to(device)\n",
    "start_epoch = 0\n",
    "\n",
    "# cache\n",
    "num_cam = 4\n",
    "pool = ThreadPoolExecutor(5)\n",
    "futures = []\n",
    "for i in range(num_cam):\n",
    "    for d in model.depths[::2]:\n",
    "        futures.append(pool.submit(sweep.get_grid, i, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup solver scheduler\n",
    "print('=> setting optimizer')\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr, momentum=args.momentum)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "\n",
    "print('=> setting scheduler')\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "if args.pretrained:\n",
    "    checkpoint = torch.load(args.pretrained)\n",
    "    param_check = {\n",
    "        'ndisp' : model.ndisp,\n",
    "        'min_depth' : model.min_depth,\n",
    "        'output_width' : model.w,\n",
    "        'output_height' : model.h,\n",
    "    }\n",
    "    for key, val in param_check.items():\n",
    "        if not checkpoint[key] == val:\n",
    "            print(f'Error! Key:{key} is not the same as the checkpoints')\n",
    "            \n",
    "    print(\"=> using pre-trained weights\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(\"=> Resume training from epoch {}\".format(start_epoch))\n",
    "    \n",
    "timestamp = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "log_folder = join('checkpoints', f'{args.arch}_{timestamp}')\n",
    "print(f'=> create log folder: {log_folder}')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "with open(join(log_folder, 'args.json'), 'w') as f:\n",
    "    json.dump(vars(args), f, indent=1)\n",
    "writer = SummaryWriter(log_dir=log_folder)\n",
    "    \n",
    "print('=> wait for a while until all tasks in pool are finished')\n",
    "pool.shutdown()\n",
    "print('=> Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cache\n",
    "# with torch.no_grad():\n",
    "#     for key in batch.keys():\n",
    "#         batch[key] = batch[key].to(device)\n",
    "# #     out = model(batch)\n",
    "# # #     del out, batch # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single batch overfitting\n",
    "# from tqdm.notebook import tqdm\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # collect few batch\n",
    "# batchs = []\n",
    "# for it in train_loader:\n",
    "#     batchs.append(it)\n",
    "#     if len(batchs) > 10:\n",
    "#         break\n",
    "        \n",
    "# # Start overfitting\n",
    "# model.train()\n",
    "# losses = []\n",
    "# pbar = tqdm(range(1000))\n",
    "# for it in pbar:\n",
    "#     batch = random.choice(batchs)\n",
    "#     # to cuda\n",
    "#     for key in batch.keys():\n",
    "#         batch[key] = batch[key].to(device)\n",
    "#     pred = model(batch)\n",
    "\n",
    "#     gt_idepth = batch['idepth']\n",
    "#     # Loss function\n",
    "#     gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "#     loss = nn.L1Loss()(pred, gt_invd_idx)\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "#     # update parameters\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # update progress bar\n",
    "#     display = OrderedDict(it=f\"{it:>2}\", loss=f\"{losses[-1]:.4f}\")\n",
    "#     pbar.set_postfix(display)\n",
    "\n",
    "# plt.title('Loss (log)')\n",
    "# plt.plot(losses)\n",
    "# plt.yscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # to cuda\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        pred = model(batch)\n",
    "\n",
    "        gt_idepth = batch['idepth']\n",
    "        # Loss function  \n",
    "        gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "        loss = nn.L1Loss()(pred, gt_invd_idx)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        display = OrderedDict(epoch=f\"{epoch:>2}\",loss=f\"{losses[-1]:.4f}\")\n",
    "        pbar.set_postfix(display)\n",
    "        \n",
    "        # tensorboard log\n",
    "        if idx % args.log_interval == 0:\n",
    "            niter = epoch*len(train_loader)+idx\n",
    "            writer.add_scalar('train/loss', loss.item(), niter)\n",
    "        if idx % 100*args.log_interval == 0:\n",
    "            niter = epoch*len(train_loader)+idx\n",
    "            imgs = []\n",
    "            for cam in model.cam_list:\n",
    "                imgs.append(0.5*batch[cam][0]+0.5)\n",
    "            img_grid = make_grid(imgs, nrow=2, padding=5, pad_value=1)\n",
    "            writer.add_image('train/fisheye', img_grid, niter)\n",
    "            writer.add_image('train/pred', pred/model.ndisp, niter)\n",
    "            writer.add_image('train/gt', gt_invd_idx/model.ndisp, niter)\n",
    "    \n",
    "    # End of one epoch\n",
    "    scheduler.step()\n",
    "    ave_loss = sum(losses)/len(losses)\n",
    "    writer.add_scalar('train/loss_ave', ave_loss, epoch)\n",
    "    print(f\"Epoch:{epoch}, Loss average:{ave_loss:.4f}\")\n",
    "    \n",
    "    save_data = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'ave_loss' : ave_loss,\n",
    "        'ndisp' : model.ndisp,\n",
    "        'min_depth' : model.min_depth,\n",
    "        'output_width' : model.w,\n",
    "        'output_height' : model.h,\n",
    "    }\n",
    "    torch.save(save_data, join(log_folder, f'checkpoints_{epoch}.pth'))\n",
    "    \n",
    "    plt.title(f'epoch {epoch}:Loss (log)')\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    root_dir = '../datasets/omnithings'\n",
    "    filename_list = 'omnithings_val.txt'\n",
    "else:\n",
    "    root_dir = '../datasets/omnihouse'\n",
    "    filename_list = 'omnihouse_val.txt'\n",
    "    \n",
    "valset = OmniStereoDataset(root_dir, filename_list, transform=train_transform)\n",
    "val_loader = DataLoader(valset, args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "loader_iter = iter(val_loader)\n",
    "# loader_iter = iter(train_loader)\n",
    "print(filename_list)\n",
    "print('val_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = batchs[3]#\n",
    "batch = loader_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "    pred = model(batch)\n",
    "    gt_idepth = batch['idepth']\n",
    "    gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "    error = torch.abs(pred-gt_invd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for cam in model.cam_list:\n",
    "    imgs.append(0.5*batch[cam][0]+0.5)\n",
    "img_grid = ToPIL(make_grid(imgs, padding=5, pad_value=1))\n",
    "\n",
    "pred_vis = ToPIL(pred/args.ndisp)\n",
    "gt_vis = ToPIL(gt_invd_idx/args.ndisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap='viridis'\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12,12), subplot_kw=({\"xticks\":(), \"yticks\":()}))\n",
    "ax[0].set_title('fisheye images')\n",
    "ax[0].imshow(img_grid)\n",
    "ax[1].set_title('prediction')\n",
    "ax[1].imshow(pred_vis, cmap=cmap)\n",
    "ax[2].set_title('groudtruth')\n",
    "ax[2].imshow(gt_vis, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------Experimental from here -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as Rot\n",
    "def convertPoseToOmniMVS(Twcs, filename):\n",
    "    rot_t_vecs = []\n",
    "    for T in Twcs:\n",
    "        # rot\n",
    "        R = T[:3,:3]\n",
    "        rotvec = Rot.from_matrix(R).as_rotvec()\n",
    "        # tvec m -> cm\n",
    "        tvec = T[:3, 3]*100\n",
    "        rot_t_vecs.append(np.concatenate((rotvec, tvec)))\n",
    "    rot_t_vecs = np.stack(rot_t_vecs)\n",
    "    np.savetxt(filename, rot_t_vecs, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to OmniMVS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load camera poses\n",
    "data_folder = \"../real_data/\"\n",
    "fov = 185\n",
    "fs_read = cv2.FileStorage(join(data_folder, \"final_camera_poses.yml\"), cv2.FILE_STORAGE_READ)\n",
    "Twcs = []\n",
    "for i in range(4):\n",
    "    # get world <- cam transformation\n",
    "    key = f'originimg{i}'\n",
    "    Twcs.append(fs_read.getNode(key).mat())\n",
    "    \n",
    "# ocamcalib filenames in data_folder\n",
    "ocam_files = [\n",
    "    'calib_results_0.txt',\n",
    "    'calib_results_1.txt',\n",
    "    'calib_results_2.txt',\n",
    "    'calib_results_3.txt'\n",
    "]\n",
    "img_files = [\n",
    "    'img0.jpg',\n",
    "    'img1.jpg',\n",
    "    'img2.jpg',\n",
    "    'img3.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to OmniMVS format\n",
    "convertPoseToOmniMVS(Twcs, join(data_folder, 'poses.txt'))\n",
    "\n",
    "# convert to OmniMVS filename\n",
    "import shutil\n",
    "for i, it in enumerate(ocam_files):\n",
    "    src = join(data_folder, it)\n",
    "    dst = join(data_folder, f'ocam{i+1}.txt')\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change sweeping module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sweep = SphericalSweeping(data_folder, h=model.h, w=model.w, fov=fov)\n",
    "model.sweep = new_sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([Resize((500, 500), depth_size), ToTensor(), Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "batch = {}\n",
    "for i in range(4):\n",
    "    cam = model.cam_list[i]\n",
    "    fname = join(data_folder, img_files[i])\n",
    "    valid = model.sweep.valid_area(i)\n",
    "    batch[cam] = load_image(fname, valid=valid)\n",
    "    \n",
    "batch = transform(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "        if batch[key].dim() == 3:\n",
    "            batch[key].unsqueeze_(0)\n",
    "    pred = model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for cam in model.cam_list:\n",
    "    imgs.append(0.5*batch[cam][0]+0.5)\n",
    "img_grid = ToPIL(make_grid(imgs, padding=5, pad_value=1))\n",
    "\n",
    "pred_vis = ToPIL(pred/args.ndisp)\n",
    "# gt_vis = ToPIL(gt_invd_idx/args.ndisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inputs = {}\n",
    "vis_outputs = {}\n",
    "\n",
    "def vis_hook(m, i, o, name):\n",
    "    vis_inputs[name] = i[0]\n",
    "    vis_outputs[name] = o\n",
    "    \n",
    "# add hook\n",
    "model.transference.register_forward_hook(partial(vis_hook, name='transference'))\n",
    "model.fusion.register_forward_hook(partial(vis_hook, name='fusion'))\n",
    "model.cost_regularization.register_forward_hook(partial(vis_hook, name='cost_reg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)\n",
    "    pred = model(batch)\n",
    "    gt_idepth = batch['idepth']\n",
    "    gt_invd_idx = converter.invdepth_to_index(gt_idepth)\n",
    "    error = torch.abs(pred-gt_invd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inputs['transference'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd_idx = 9\n",
    "vis_tensor = vis_inputs['transference'][0, :, invd_idx].unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inputs['fusion'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd_idx = 0\n",
    "vis_tensor = vis_inputs['fusion'][0, :, invd_idx].unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1, normalize=True)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invd_idx = 4\n",
    "vis_tensor = vis_outputs['fusion'][0, :, invd_idx].unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1, normalize=True)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_outputs['cost_reg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_tensor = vis_outputs['cost_reg'][0,0,:,:64,:].transpose(0, 1)\n",
    "vis_tensor = vis_tensor.unsqueeze(1)\n",
    "grid_img = make_grid(vis_tensor, padding=5, pad_value=1, normalize=True)\n",
    "ToPIL(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
